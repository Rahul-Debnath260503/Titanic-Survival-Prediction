# -*- coding: utf-8 -*-
"""TITANIC_SURVIVAL_PREDICTION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b4SMUdCv_rsrHC3oBWv_K4HoJu3_Yoyo

** TITANIC SURVIVAL PREDICTION**

TASK:- Use the Titanic dataset to build a model that predicts whether a
 passenger on the Titanic survived or not.
"""

import numpy as np
import pandas as pd
df=pd.read_csv('/content/Titanic-Dataset.csv')

"""#Explore the dataframe

Extract the head of the dataset.
"""

df.head()

"""Extract the tail of the dataset."""

df.tail()

"""**Information about the dataset.To get a concise summary of the DATASET.**"""

df.info()

"""#Generate summary statistics for numerical columns in a DataFrame."""

df.describe()

"""Count the non-null (non-missing) values in each column of the DataFrame."""

df.count()

"""Shape of the DataFrame"""

df.shape

"""#Visualization the data for better understanding."""

import matplotlib.pyplot as plt
import seaborn as sns

sns.set_style("whitegrid")

# 1st Countplot for Survival Distribution.
plt.figure(figsize=(10,6))
sns.countplot(x="Survived", data=df, palette="coolwarm")
plt.title("Survival Count")
plt.xlabel("Survived (1=YES, 0=NO)")
plt.ylabel("Count")
plt.show()

#2nd count plot for gender distribution

plt.figure(figsize=(10,6))
sns.countplot(x="Sex", data=df, palette="coolwarm")
plt.title("Gender Distribution")
plt.xlabel("Sex")
plt.ylabel("Count")
plt.show()

#3rd survival rate by gender

plt.figure(figsize=(10,6))
sns.barplot(x="Sex", y="Survived",legend=False, data=df, palette="coolwarm")
plt.title("survival rate by gender")
plt.xlabel("Sex")
plt.ylabel("Survival")
plt.show()

#4th histplot of Age distribution
plt.figure(figsize=(10,6))
sns.histplot(df["Age"].dropna(), bins=30, kde=True, color="purple")
plt.title("Age Distribution")
plt.xlabel("Age")
plt.ylabel("Frequency")
plt.show()

#5th survival rate by passenger class
plt.figure(figsize=(10,6))
sns.countplot(x="Pclass", hue="Survived", data=df, palette="coolwarm")
plt.title("Survival Count by Passenger Class")
plt.xlabel("Passenger Class")
plt.ylabel("Count")
plt.show()

#6th Fare Distribution
plt.figure(figsize=(10,6))
sns.histplot(df["Fare"], bins=30,kde=True, color="Green")
plt.title("Fare Distribution")
plt.xlabel("Fare")
plt.ylabel("Count")
plt.show()

#7th pairplot for numerical features
sns.pairplot(df[["Survived", "Age", "Fare", "Pclass", "SibSp", "Parch"]],hue="Survived",palette="coolwarm")
plt.show()

"""# Deals with Missing values"""

#checking missing values

print(df.isnull().sum())

"""# **Handle missing values**

**The cabin column has so many missing values(687 out of 891). since most are missing, so we drop this column.**
"""

df.drop(columns=["Cabin"], inplace=True)

"""**Fill Missing Values for Age column. because it has 177 missing values. so we can fill them with median.**"""

df.fillna({"Age": df["Age"].median()}, inplace=True)

"""**Fill Missing Values for Embarked**
**The Embarked column has only 2 missing values. Since Embarked is categorical, we replace missing values with the most frequent (mode) value**
"""

df["Embarked"] = df["Embarked"].fillna(df["Embarked"].mode()[0])

df.count()

"""# Verify Missing Values Are Handled or Not"""

print(df.isnull().sum())

"""# Encoding Categorical Variables.

#1st: Sex has only two unique values [male, female], we use Label Encoding [male → 0, female → 1]

"""

df["Sex"]= df['Sex'].map({"male":0, "female":1})

"""# 2nd: One-Hot Encoding (For Multi-Class Categories)

**For Embarked (C, Q, S), we use One-Hot Encoding to create separate columns**

Here we dropping one category(Embarked C) to avoid multicollinearity(dummy variable trap).
"""

df = pd.get_dummies(df, columns=["Embarked"],drop_first=True)

"""**Check the updated DataFrame**"""

df.head()

#encode the Embarked colums True False to 1 and 0
df[['Embarked_Q' ,'Embarked_S']]=df[['Embarked_Q' , 'Embarked_S']].astype(int)

"""#Identify Numerical Columns for Scaling

**Some numerical columns have very different ranges:**

Age: Typically between (0–80),  
Fare: Can be as high as : 500+ ,  
Pclass, SibSp, Parch: Small integer values - (1, 2, 3, etc.)
To ensure all features contribute equally to the model, i have to scale them.

# Choose a Scaling Method

**We have two common methods:**

1st Standardization (StandardScaler) *Centers data around 0 with a standard deviation of 1. , *Good for normally distributed data.

2nd Normalization (MinMaxScaler) *Scales values between 0 and 1 , *Good for non-Gaussian or skewed data.

**Since Fare and Age columns are maybe skewed, MinMaxScaler is a better choice for this project.**

#Apply Scaling Method
"""

from sklearn.preprocessing import MinMaxScaler

##select numerical columns to scale
num_cols = ['Age', 'Fare', 'SibSp', 'Parch']

#initialize scaler
scaler= MinMaxScaler()

#fit and transform the selected columns
df[num_cols]= scaler.fit_transform(df[num_cols])

#result
print(df.head())

"""#Define feature and target variables

**Features (X): Independent variables, columns=["Survived", "PassengerId", "Name", "Ticket"]

Target (y): Survived column (0 = died, 1 = survived)


"""

## Define target variable
y = df['Survived'].copy()
print(type(y))
print(y.shape)

## Define features (drop target and irrelevant columns)
x = df.drop(columns=['Survived', 'PassengerId','Name','Ticket']) ## Drop non-useful columns

"""#Verify the changes"""

print(x.head())#view first row
print(x.columns)# view remaining columns names

"""#Train-Test Split

**Now X (features) and y (target) are ready, split them into training (80%) and testing (20%) datasets**
"""

#check that both x and y is a pandas dataframe or  a list object.
#if it is a pandas dataframe it will properly execute but if it is a list object it will not execute and we have to convert it in pandas dataframe
print(type(x))
print(type(y))

#check the shape of the dataframe is both x and y is same or not.
#if it is same it will execute properly otherwise not execute.
print(x.shape)
print(y.shape)

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42, stratify = y)

print("Train data shape:", x_train.shape)
print("Test data shape:", x_test.shape)

print(x_train.isnull().sum())  # Check missing values in training set
print(x_test.isnull().sum())   # Check missing values in test set

"""# Choose and Train a Machine Learning Model

**LOGISTIC REGRESSION**

When we want to predict a output that will answered in yes(1) or No(0) , that time logistic regression is a perfect model for simple baseline.

#Import and Train Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

model = LogisticRegression(max_iter=1000)
model.fit(x_train, y_train)


#make prediction on the test set
y_pred = model.predict(x_test)

#calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Print classification report (precision, recall, f1-score)
print("Classification Report:\n", classification_report(y_test, y_pred))

# Print confusion matrix
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

"""# Conclusion

**This project aimed to predict the survival of passengers aboard the Titanic using machine learning. By exploring the dataset, handling missing values, encoding categorical variables, and scaling numerical features, a Logistic Regression model was trained and evaluated. The model achieved an accuracy of approximately 80%, demonstrating its ability to make reasonable predictions based on passenger characteristics.**

**The analysis revealed key factors influencing survival, such as gender (females had a higher survival rate), passenger class (higher classes had better chances), and age (younger passengers were more likely to survive). These insights align with historical accounts of the disaster.**

**Overall, this project demonstrates the power of data science and machine learning in extracting meaningful insights from historical events and making predictions. The developed model can serve as a foundation for further research and analysis related to the Titanic disaster.**
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile README.md
# 
# # Titanic Survival Prediction
# 
# This project aims to predict the survival of passengers aboard the Titanic using machine learning. By exploring the dataset, handling missing values, encoding categorical variables, and scaling numerical features, a Logistic Regression model was trained and evaluated. The model achieved an accuracy of approximately 80%, demonstrating its ability to make reasonable predictions based on passenger characteristics.
# 
# ## Project Overview
# 
# The Titanic dataset contains information about passengers who were on board the Titanic, including their demographics, ticket details, and survival status. The goal of this project is to build a model that can accurately predict whether a passenger survived or not based on their features.
# 
# ## Steps Involved
# 
# 1. **Data Exploration and Cleaning:**
#    - Explored the dataset to understand its structure and characteristics.
#    - Handled missing values by imputing or removing them.
#    - Encoded categorical variables into numerical representations.
#    - Scaled numerical features to ensure they contribute equally to the model.
# 
# 2. **Model Selection and Training:**
#    - Chose Logistic Regression as the machine learning model for this classification task.
#    - Trained the model on a portion of the dataset (training set).
# 
# 3. **Model Evaluation:**
#    - Evaluated the model's performance on a separate portion of the dataset (testing set).
#    - Used metrics like accuracy, precision, recall, and F1-score to assess the model's effectiveness.
# 
# ## Results
# 
# The Logistic Regression model achieved an accuracy of approximately 80% in predicting passenger survival. This indicates that the model is able to make reasonable predictions based on the given features.
# 
# ## Insights
# 
# The analysis revealed key factors influencing survival, such as:
# 
# - **Gender:** Females had a higher survival rate.
# - **Passenger Class:** Higher classes had better chances of survival.
# - **Age:** Younger passengers were more likely to survive.
# 
# These insights align with historical accounts of the disaster.
# 
# ## Conclusion
# 
# This project demonstrates the power of data science and machine learning in extracting meaningful insights from historical events and making predictions. The developed model can serve as a foundation for further research and analysis related to the Titanic disaster.
# 
# 
# ## Usage
# 
# To run this project, you will need the following:
# 
# - Python 3.x
# - Jupyter Notebook
# - Pandas
# - NumPy
# - Scikit-learn
# - Matplotlib
# - Seaborn
# 
# You can install these packages using pip:
# 
# Once you have installed the necessary packages, you can open the  Google colab or Jupyter Notebook file and run the code cells.
#